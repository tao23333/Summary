## 如何防止模型的过拟合

针对问题：神经网络学习的权值对给定的数据更加专门化，而不能学习一般化的特征

正则化：正则化可以定义为我们为了减少泛化误差而不是减少训练误差而对训练算法所做出的任何改变；有很多正则化策略：对模型添加额外的约束，对目标函数添加额外的项等

在事件中使用的正则化技术有：

- L1正则化
- L2正则化
- 数据增强
- Dropout
- Early Stopping

### Loss加正则化项

- L1正则化项：各个权重参数绝对值求和
- L2正则化项：各个权重参数的平方和
  - 在使用SGD做优化时，L2正则与权重衰减是等价的

> 为什么添加正则化项会防止过拟合？
>
> ​		模型的复杂度与参数向量有关。参数越多，模型相对就越复杂；参数越少，模型就相对越简单；
>
> 所以，我们要使得某些权重参数=0或者趋向于0，这样相当于参数个数减少了，模型的复杂度也就下降了；
>
> ​		通过求导可知，正则化项会使得网络权重更小；权重更小意味着，如果我们在这里或者那里改变一些随机输入，网络的行为不会有太大的变化，这反过来使正则化的网络很难学习数据中的局部噪声。这迫使网络只学习那些在训练集中经常看到的特征；
>
> ​		直觉上看：简单地从优化损失函数的角度来考虑L2正则化，当我们把正则化项添加到损失函数中，我们实际上增加了损失函数的值，权重越大，损失也就越高；训练算法会试图通过惩罚权值来降低损失函数，迫使他们取的更小的值，从而使得网络正则化



L1正则具有稀疏性：

- 等价于说L1正则可以使得很多权重参数=0，这也是其能进行特征选择的原因





















